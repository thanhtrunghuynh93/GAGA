{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9bb9a13-ad58-46d2-9d2e-98017b548d2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict, Counter\n",
    "import numpy as np\n",
    "from textblob import TextBlob\n",
    "import pickle\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ddf74d29-1438-4c9c-aeb0-1ae63d19594e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import dgl\n",
    "from dgl.data.utils import save_graphs, load_graphs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "56db6c74-fc45-49f2-a77f-52d40b1614af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = 'Snopes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a51d47f6-ce27-4e00-90e3-15c7be821a10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(f'formatted_data/declare/{dataset}/mapped_data/5fold/train_0.tsv', delimiter='\\t')\n",
    "df_test = pd.read_csv(f'formatted_data/declare/{dataset}/mapped_data/5fold/test_0.tsv', delimiter='\\t')\n",
    "df_dev = pd.read_csv(f'formatted_data/declare/{dataset}/mapped_data/dev.tsv', delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "be7b5297-5a71-4473-b443-8c70e2caed09",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.concat([df_train, df_test, df_dev])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "54bffe04-5f6f-46ed-87e3-a6f9ca8a9649",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nli_model = AutoModelForSequenceClassification.from_pretrained('facebook/bart-large-mnli').cuda()\n",
    "tokenizer = AutoTokenizer.from_pretrained('facebook/bart-large-mnli')\n",
    "\n",
    "def score(sequence1, sequence2):\n",
    "\n",
    "    # run through model pre-trained on MNLI\n",
    "    x = tokenizer.encode(sequence1, sequence2, return_tensors='pt',\n",
    "                      truncation_strategy='only_first')\n",
    "    logits = nli_model(x.cuda())[0]\n",
    "\n",
    "    # we throw away \"neutral\" (dim 1) and take the probability of\n",
    "    # \"entailment\" (2) as the probability of the label being true \n",
    "    entail_contradiction_logits = logits[:,[0,2]]\n",
    "    probs = entail_contradiction_logits.softmax(dim=1)\n",
    "    prob_label_is_true = probs[:,1]\n",
    "    return prob_label_is_true.detach().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fbc1c2ec-c3a4-4d59-a8d1-72122d5fdf92",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                 | 0/29242 [00:00<?, ?it/s]/home/dunght/anaconda3/envs/new/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2335: FutureWarning: The `truncation_strategy` argument is deprecated and will be removed in a future version, use `truncation=True` to truncate examples to a max length. You can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to truncate to the maximal input size of the model (e.g. 512 for Bert).  If you have pairs of inputs, you can give a specific truncation strategy selected among `truncation='only_first'` (will only truncate the first sentence in the pairs) `truncation='only_second'` (will only truncate the second sentence in the pairs) or `truncation='longest_first'` (will iteratively remove tokens from the longest sentence in the pairs).\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████| 29242/29242 [14:52<00:00, 32.76it/s]\n"
     ]
    }
   ],
   "source": [
    "df['score'] = df.progress_apply(lambda row : score(row.claim_text, row.evidence), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c2e9d04b-7f53-41dd-a416-5cd0a8ea28a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dict = pickle.load(open(f'./embedding_dicts_{dataset}_fold_0.pkl','rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "554b20bd-a334-4ae5-922a-791421276bc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "29242it [00:01, 16167.23it/s]\n"
     ]
    }
   ],
   "source": [
    "claim_feats, claim_labels, claim_texts = [], [], []\n",
    "claim_ori_ids = []\n",
    "claim_ids = {}\n",
    "claim_labels = []\n",
    "src_support_claims = defaultdict(set)\n",
    "src_not_support_claims = defaultdict(set)\n",
    "\n",
    "for _, record in tqdm(df.iterrows()):\n",
    "    if record.id_left not in claim_ids:\n",
    "        claim_texts.append(record.claim_text)\n",
    "        claim_feats.append(embedding_dict[record.id_left])\n",
    "        claim_labels.append(int(record.cred_label))\n",
    "        claim_ids[record.id_left] = len(claim_ids)\n",
    "        claim_ori_ids.append(record.id_left)\n",
    "    claim_id = claim_ids[record.id_left]\n",
    "\n",
    "    if record.score < 0.5:\n",
    "        src_not_support_claims[record.evidence_source].add(claim_id)\n",
    "    else:\n",
    "        src_support_claims[record.evidence_source].add(claim_id)\n",
    "claim_features = np.array(claim_feats)\n",
    "claim_labels = np.array(claim_labels)\n",
    "claim_ori_ids = np.array(claim_ori_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "57ac4060-dfd5-4653-9621-fb3d825d398a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for src in list(src_support_claims):\n",
    "    src_support_claims[src] = list(src_support_claims[src])\n",
    "for src in list(src_not_support_claims):\n",
    "    src_not_support_claims[src] = list(src_support_claims[src])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9cfdb522-eab9-42fd-9a22-0045819c85f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "edges = defaultdict(set)\n",
    "for src in src_support_claims:\n",
    "    claims =  src_support_claims[src]\n",
    "    for i in range(len(claims)):\n",
    "        for j in range(i + 1, len(claims)):\n",
    "            edges['1-1'].add((claims[i], claims[j]))\n",
    "            edges['1-1'].add((claims[j], claims[i]))\n",
    "for src in src_not_support_claims:\n",
    "    claims = src_not_support_claims[src]\n",
    "    for i in range(len(claims)):\n",
    "        for j in range(i + 1, len(claims)):\n",
    "            edges['0-0'].add((claims[i], claims[j]))\n",
    "            edges['0-0'].add((claims[j], claims[i]))\n",
    "for src in set(src_not_support_claims).intersection(src_support_claims):\n",
    "    claims_1 = src_support_claims[src]\n",
    "    claims_0 = src_not_support_claims[src]\n",
    "    for claim_1 in claims_1:\n",
    "        for claim_0 in claims_0:\n",
    "            edges['1-0'].add((claim_1, claim_0))\n",
    "            edges['0-1'].add((claim_0, claim_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9d4a2325-5b3d-4381-b424-700701d88941",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for claims_ in src_support_claims.values():\n",
    "    for i in range(len(claims_)):\n",
    "        for j in range(i + 1, len(claims_)):\n",
    "            edges['mutual_evd'].add((claims_[i], claims_[j]))\n",
    "            edges['mutual_evd'].add((claims_[j], claims_[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a810a183-31ea-4dde-9261-7391554a05a7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4341it [00:00, 4950.97it/s]\n"
     ]
    }
   ],
   "source": [
    "term_to_claim_ids = defaultdict(set)\n",
    "for claim_id, claim_text in tqdm(enumerate(claim_texts)):\n",
    "    text = TextBlob(claim_text)\n",
    "    for term in text.noun_phrases:\n",
    "        term_to_claim_ids[term].add(claim_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7ea3e0f7-77a7-4bb1-8060-75cff6320db1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for claim_ids_ in term_to_claim_ids.values():\n",
    "    claim_ids_ = list(claim_ids_)\n",
    "    for i in range(len(claim_ids_)):\n",
    "        for j in range(i + 1, len(claim_ids_)):\n",
    "            edges['np'].add((claim_ids_[i], claim_ids_[j]))\n",
    "            edges['np'].add((claim_ids_[j], claim_ids_[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "77634122-11f9-4bf0-b7c2-0021d836ae2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for etype in list(edges):\n",
    "    edges[etype] = np.array([list(e) for e in edges[etype]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "46bf1224-206b-45a9-b25f-58e13c063695",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 5/5 [00:01<00:00,  4.70it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(5)):\n",
    "    df_train = pd.read_csv(f'formatted_data/declare/{dataset}/mapped_data/5fold/train_{i}.tsv', delimiter='\\t')\n",
    "    df_test = pd.read_csv(f'formatted_data/declare/{dataset}/mapped_data/5fold/test_{i}.tsv', delimiter='\\t')\n",
    "    df_dev = pd.read_csv(f'formatted_data/declare/{dataset}/mapped_data/dev.tsv', delimiter='\\t')\n",
    "    train_ids, val_ids, test_ids = set(df_train.id_left), set(df_dev.id_left), set(df_test.id_left)\n",
    "    train_mask, val_mask, test_mask = [], [], []\n",
    "    for id_left in claim_ori_ids:\n",
    "        is_train, is_val, is_test = 0, 0, 0\n",
    "        if id_left in train_ids:\n",
    "            is_train = 1\n",
    "        elif id_left in test_ids:\n",
    "            is_test = 1\n",
    "        else:\n",
    "            is_val = 1\n",
    "        train_mask.append(is_train)\n",
    "        val_mask.append(is_val)\n",
    "        test_mask.append(is_test)\n",
    "        \n",
    "    graphs = []\n",
    "    for ignored_rels in [['mutual_evd'], \n",
    "                        ['mutual_evd', 'np'],\n",
    "                        ['1-1', '0-0', '1-0', '0-1'],\n",
    "                        ['1-1', '0-0', '1-0', '0-1', 'np']]:\n",
    "        g = dgl.heterograph({\n",
    "            ('claim', etype, 'claim') : (merged_edges_[:, 0], merged_edges_[:, 1])\n",
    "            for etype, merged_edges_ in edges.items() if etype not in ignored_rels\n",
    "        }, num_nodes_dict = {'claim' : len(claim_labels)}).to('cuda')\n",
    "        g.ndata['train_mask'] = torch.BoolTensor(train_mask).cuda()\n",
    "        g.ndata['val_mask'] = torch.BoolTensor(val_mask).cuda()\n",
    "        g.ndata['test_mask'] = torch.BoolTensor(test_mask).cuda()\n",
    "        g.ndata['label'] = torch.LongTensor(claim_labels).cuda()\n",
    "        g.ndata['feature'] = torch.FloatTensor(claim_features).cuda()\n",
    "        graphs.append(g)\n",
    "    save_graphs(f\"../GAGA/pytorch_gaga/preprocessing/{dataset}_{i}.bin\", graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "104e3007-008b-4286-b0b5-e6c97cd6f717",
   "metadata": {},
   "outputs": [],
   "source": [
    "gs = load_graphs('../GAGA/pytorch_gaga/preprocessing/Snopes_0.bin')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "becd4177-90c5-4f36-973d-ac1146b39f9e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0-0', '0-1', '1-0', '1-1', 'np']\n",
      "['0-0', '0-1', '1-0', '1-1']\n",
      "['mutual_evd', 'np']\n",
      "['mutual_evd']\n"
     ]
    }
   ],
   "source": [
    "for g in gs:\n",
    "    print(g.etypes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new",
   "language": "python",
   "name": "new"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
